# GitHub Repository Metrics Example

## Overview

This example demonstrates a data workflow that integrates multiple data processing systems:

1. Fetches repository data from the GitHub API
2. Processes and analyzes the data in DuckDB
3. Stores the data in PostgreSQL
4. Generates a comprehensive report using Python
5. Updates the PostgreSQL database with the full report

The example is designed to show how to pass data between steps using 'with' attributes and template variables, particularly demonstrating how to access results from different action types (HTTP, DuckDB, PostgreSQL, Python) and use them in subsequent steps.

## Setup Options

### Option 1: Local Setup

1. Set up environment variables for Postgres connection:
```bash
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5432
export POSTGRES_USER=demo
export POSTGRES_PASSWORD=demo
export POSTGRES_DB=demo_noetl
```

2. Execute the workflow:
```bash
noetl playbooks --execute --path "examples/github/github_metrics_example"
```

### Option 2: Docker Setup

You can run the example in a Docker container using the following commands:

```bash
# Build and start the containers
make build
make up

# Execute the workflow
docker exec -it noetl noetl playbooks --execute --path "examples/github/github_metrics_example"
```

## Prerequisites

Before running this example, ensure the following are set up:

1. **NoETL Framework**: The NoETL command-line interface (`noetl`) must be installed and configured.
2. **PostgreSQL**: A running PostgreSQL instance must be accessible. Connection details can be provided via environment variables or will use the defaults specified in the `workload` section.
3. **Internet Connection**: Required to access the GitHub API.

## Workflow Breakdown

The playbook executes a sequence of steps defined in the `workflow` section. Each step performs a specific task, passing its output to subsequent steps.

| Step | Description | Type |
| :--- | :--- | :--- |
| **1. `start`** | Initiates the workflow. | N/A |
| **2. `fetch_github_repo`** | Makes a GET request to the GitHub API to fetch information about a repository (default: "microsoft/vscode"). | `http` |
| **3. `extract_repo_metrics`** | Creates tables in DuckDB from the GitHub data and calculates various metrics like popularity tier and engagement. | `duckdb` |
| **4. `store_in_postgres`** | Creates a table in PostgreSQL and stores the repository analysis data. | `postgres` |
| **5. `query_and_analyze`** | Queries the stored data and performs additional analysis, including activity status. | `postgres` |
| **6. `generate_report`** | Accesses results from all previous steps and generates a comprehensive report with insights. | `python` |
| **7. `update_report_in_postgres`** | Updates the PostgreSQL database with the full report. | `postgres` |
| **8. `end`** | Marks the successful completion of the workflow. | N/A |

## Key Features

### 1. Data Flow Between Different Action Types

The example demonstrates a complete data flow:
- **HTTP** → **DuckDB** → **PostgreSQL** → **Python** → **PostgreSQL**

### 2. Using 'with' Attributes for Data Passing

The example shows how to pass data between steps using 'with' attributes:

- **From HTTP to DuckDB**:
  ```yaml
  with:
    repo_name: "{{ fetch_github_repo.data.name }}"
    repo_full_name: "{{ fetch_github_repo.data.full_name }}"
    stars_count: "{{ fetch_github_repo.data.stargazers_count }}"
    forks_count: "{{ fetch_github_repo.data.forks_count }}"
  ```

- **From DuckDB to PostgreSQL**:
  ```yaml
  with:
    repo_data: "{{ extract_repo_metrics.command_2.rows }}"
    stats_data: "{{ extract_repo_metrics.command_5.rows }}"
  ```

- **From PostgreSQL to Python**:
  ```yaml
  with:
    repository_info: "{{ query_and_analyze.command_10.rows }}"
    original_api_data: "{{ fetch_github_repo.data }}"
  ```

### 3. Data Processing and Analysis

The example includes:
- Repository metrics calculation in DuckDB
- Popularity tier classification
- Activity status determination
- Star-to-fork ratio analysis
- Comprehensive report generation with insights

## Configuration Details

The playbook's behavior is defined in the `workload` section:

- `jobId`, `execution_id`: Unique identifiers for the run, generated by NoETL.
- `pg_*`: PostgreSQL connection parameters. These can be overridden by setting environment variables (`POSTGRES_HOST`, `POSTGRES_PORT`, etc.).
- `api_base_url`: The base URL for the GitHub API.
- `repository`: The GitHub repository to analyze (default: "microsoft/vscode").

## Database Schema

The playbook creates a table in your PostgreSQL database:

### `github_repo_analysis`

| Column | Description |
| :--- | :--- |
| `id` | Primary key (SERIAL). |
| `repo_name` | The name of the GitHub repository. |
| `full_name` | The full name of the repository (owner/repo). |
| `stars` | Number of stars the repository has. |
| `forks` | Number of forks the repository has. |
| `primary_language` | The primary programming language used in the repository. |
| `total_engagement` | Sum of stars and forks. |
| `popularity_tier` | Classification based on star count (e.g., "Extremely Popular"). |
| `star_to_fork_ratio` | Ratio of stars to forks. |
| `days_since_creation` | Days since the repository was created. |
| `days_since_update` | Days since the repository was last updated. |
| `analysis_date` | Timestamp of when the analysis was performed. |
| `full_report` | The complete analysis report in text format. |

## Customization

This playbook is designed to be a template. You can customize it in several ways:

- **Change Target Repository**: Modify the `repository` value in the `workload` section to analyze a different GitHub repository.
- **Extend DuckDB Analysis**: Add more complex metrics or analysis in the `extract_repo_metrics` step.
- **Modify PostgreSQL Schema**: Change the table structure in the `store_in_postgres` step to store different or additional data.
- **Enhance Python Report**: Extend the Python code in the `generate_report` step to perform additional analysis or formatting.

## Files

- **Example Playbook**: `examples/github/github_metrics_example.yaml`
- **Documentation**: This README file