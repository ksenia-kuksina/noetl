# Wikipedia DuckDB PostgreSQL Example

## Overview

This example demonstrates a data workflow that integrates multiple data processing systems:

1. Fetches data from the Wikipedia API
2. Processes the data in DuckDB (including text analysis)
3. Stores the data in PostgreSQL
4. Accesses results from different action types in subsequent steps

The example is designed to show how to access results from different action types (HTTP, DuckDB, PostgreSQL) and use them in subsequent steps, particularly in a final Python step that can access and display all the results.

## Setup Options

### Option 1: Local Setup

1. Set up environment variables for PostgreSQL connection:
```bash
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5432
export POSTGRES_USER=demo
export POSTGRES_PASSWORD=demo
export POSTGRES_DB=demo_noetl
```

2. Execute the workflow:
```bash
noetl playbooks --execute --path "examples/wikipedia/wikipedia_duckdb_postgres_example"
```

### Option 2: Docker Setup

You can run the example in a Docker container using the following commands:

```bash
# Build and start the containers
make build
make up

# Execute the workflow
docker exec -it noetl noetl playbooks --execute --path "examples/wikipedia/wikipedia_duckdb_postgres_example"
```

## Prerequisites

Before running this example, ensure the following are set up:

1. **NoETL Framework**: The NoETL command-line interface (`noetl`) must be installed and configured.
2. **PostgreSQL**: A running PostgreSQL instance must be accessible. Connection details can be provided via environment variables or will use the defaults specified in the `workload` section.
3. **Internet Connection**: Required to access the Wikipedia API.

## Workflow Breakdown

The playbook executes a sequence of steps defined in the `workflow` section. Each step performs a specific task, passing its output to subsequent steps.

| Step | Description | Type |
| :--- | :--- | :--- |
| **1. `start`** | Initiates the workflow. | N/A |
| **2. `fetch_wikipedia_data`** | Makes a GET request to the Wikipedia API to fetch information about "NoSQL". | `http` |
| **3. `process_in_duckdb`** | Creates tables in DuckDB from the Wikipedia data and performs text analysis (word counting). | `duckdb` |
| **4. `create_postgres_table`** | Creates a table in PostgreSQL with the appropriate schema for storing Wikipedia article data. | `postgres` |
| **5. `insert_into_postgres`** | Inserts the Wikipedia data into the PostgreSQL table. | `postgres` |
| **6. `select_from_postgres`** | Selects data from the PostgreSQL table to demonstrate result access. | `postgres` |
| **7. `access_results_in_python`** | Accesses results from all previous steps (HTTP, DuckDB, PostgreSQL) and formats/displays the data. | `python` |
| **8. `end`** | Marks the successful completion of the workflow. | N/A |

## Key Features

### 1. Data Flow Between Different Action Types

The example demonstrates a complete data flow:
- **HTTP** → **DuckDB** → **PostgreSQL** → **Python**

### 2. Accessing Results

The example shows different ways to access results:

- **In Jinja Templates**:
  ```yaml
  '{{ fetch_wikipedia_data.data.title }}'  # Access HTTP results
  ```

- **In Python**:
  ```python
  # Access HTTP results
  wiki_title = context.get('fetch_wikipedia_data', {}).get('data', {}).get('title')
  
  # Access DuckDB results
  duckdb_results = context.get('process_in_duckdb', {}).get('data', {})
  wiki_data = duckdb_results.get('command_0', [])
  
  # Access PostgreSQL results
  postgres_results = context.get('select_from_postgres', {}).get('data', {}).get('command_0', {})
  postgres_rows = postgres_results.get('rows', [])
  ```

### 3. Data Processing

The example includes:
- Text processing in DuckDB (word counting)
- Data transformation between systems
- Structured data storage in PostgreSQL

## Configuration Details

The playbook's behavior is defined in the `workload` section:

- `jobId`, `execution_id`: Unique identifiers for the run, generated by NoETL.
- `pg_*`: PostgreSQL connection parameters. These can be overridden by setting environment variables (`POSTGRES_HOST`, `POSTGRES_PORT`, etc.).
- `table_name`: The name of the PostgreSQL table where Wikipedia data will be stored.

## Database Schema

The playbook creates a table in your PostgreSQL database:

### `wikipedia_articles`

| Column | Description |
| :--- | :--- |
| `id` | Primary key (SERIAL). |
| `title` | The title of the Wikipedia article. |
| `description` | A short description of the article. |
| `extract` | The main text extract from the article. |
| `last_updated` | Timestamp of when the article was last updated. |
| `created_at` | Timestamp of when the record was inserted. |

## Customization

This playbook is designed to be a template. You can customize it in several ways:

- **Change Wikipedia Article**: Modify the endpoint URL in the `fetch_wikipedia_data` step to fetch a different Wikipedia article.
- **Extend DuckDB Analysis**: Add more complex text analysis or data processing steps in the `process_in_duckdb` step.
- **Modify PostgreSQL Schema**: Change the table structure in the `create_postgres_table` step to store different or additional data.
- **Enhance Python Processing**: Extend the Python code in the `access_results_in_python` step to perform additional analysis or formatting.

## Files

- **Example Playbook**: `examples/wikipedia/wikipedia_duckdb_postgres_example.yaml`
- **Documentation**: This README file
