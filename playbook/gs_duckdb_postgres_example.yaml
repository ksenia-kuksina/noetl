# NoETL DSL Playbook for Google Storage, DuckDB, and PostgreSQL operations
# Doc: docs/examples/gs_duckdb_postgres_example.md
# Usage:
# noetl playbook --register playbook/gs_duckdb_postgres_example.yaml --port 8080
# noetl playbook --execute --path "workflows/examples/gs_duckdb_postgres_example"

apiVersion: noetl.io/v1
kind: Playbook
name: gs_duckdb_postgres_example
path: workflows/examples/gs_duckdb_postgres_example

workload:
  jobId: "{{ job.uuid }}"
  execution_id: "{{ job.uuid }}"
  gs_project_id: "{{ env.GOOGLE_CLOUD_PROJECT | default('noetl-demo-19700101') }}"
  gs_bucket: "noetl-demo-19700101"
  gcs_key_id_secret: "{{ env.GCS_KEY_ID_SECRET | default('s3_access_key_id') }}"
  gcs_secret_key_secret: "{{ env.GCS_SECRET_KEY_SECRET | default('s3_secret_access_key') }}"
  pg_user_secret: "{{ env.POSTGRES_USER_SECRET | default('postgres_user') }}"
  pg_password_secret: "{{ env.POSTGRES_PASSWORD_SECRET | default('postgres_password') }}"
  source_csv_path: "data/test/test_data.csv"
  local_csv_path: "/tmp/test_data.csv"
  local_parquet_path: "/tmp/test_data.parquet"
  gs_csv_path: "uploads/test_data.csv"
  gs_parquet_path: "uploads/test_data.parquet"
  pg_host: "{{ env.POSTGRES_HOST | default('localhost') }}"
  pg_port: "{{ env.POSTGRES_PORT | default('5434') }}"
  pg_user: "{{ env.POSTGRES_USER | default('noetl') }}"
  pg_password: "{{ env.POSTGRES_PASSWORD | default('noetl') }}"
  pg_db: "{{ env.POSTGRES_DB | default('noetl') }}"
  source_table_name: "test_data_table"
  table_name: "test_data"

workflow:
  - step: start
    desc: "Start GS DuckDB Postgres Example Workflow"
    next:
      - step: get_gcs_key_id

  - step: get_gcs_key_id
    desc: "Retrieve GCS HMAC Key ID from Google Secret Manager"
    call:
      type: secrets
      provider: google
      secret_name: "{{ workload.gcs_key_id_secret }}"
    next:
      - step: get_gcs_secret_key

  - step: get_gcs_secret_key
    desc: "Retrieve GCS HMAC Secret Key from Google Secret Manager"
    call:
      type: secrets
      provider: google
      secret_name: "{{ workload.gcs_secret_key_secret }}"
    next:
      - step: get_pg_user

  - step: get_pg_user
    desc: "Retrieve PostgreSQL User from Google Secret Manager"
    call:
      type: secrets
      provider: google
      secret_name: "{{ workload.pg_user_secret }}"
    next:
      - step: get_pg_password

  - step: get_pg_password
    desc: "Retrieve PostgreSQL Password from Google Secret Manager"
    call:
      type: secrets
      provider: google
      secret_name: "{{ workload.pg_password_secret }}"
    next:
      - step: create_gcs_secret

  - step: create_gcs_secret
    desc: "Create Duckdb secret for GCS authentication"
    call:
      type: workbook
      name: create_gcs_secret_task
      with:
        key_id: "{{ get_gcs_key_id.secret_value }}"
        secret_key: "{{ get_gcs_secret_key.secret_value }}"
    next:
      - step: create_pg_secret

  - step: create_pg_secret
    desc: "Create Duckdb secret for PostgreSQL authentication"
    call:
      type: workbook
      name: create_pg_secret_task
      with:
        pg_user: "{{ get_pg_user.secret_value }}"
        pg_password: "{{ get_pg_password.secret_value }}"
        execution_id: "{{ workload.execution_id }}"
    next:
      - step: test_gcs_credentials

  - step: test_gcs_credentials
    desc: "Test GCS credentials by attempting to list bucket or create a test file"
    call:
      type: workbook
      name: test_gcs_credentials
      with:
        key_id: "{{ get_gcs_key_id.secret_value }}"
        secret_key: "{{ get_gcs_secret_key.secret_value }}"
    next:
      - step: read_from_postgres

  - step: read_from_postgres
    desc: "Read data from PostgreSQL test_data_table and store on local filesystem"
    call:
      type: workbook
      name: read_from_postgres_task
      with:
        pg_user: "{{ get_pg_user.secret_value }}"
        pg_password: "{{ get_pg_password.secret_value }}"
        execution_id: "{{ workload.execution_id }}"
    next:
      - step: upload_csv_to_gs

  - step: upload_csv_to_gs
    desc: "Upload CSV file to Google Storage bucket"
    call:
      type: workbook
      name: upload_csv_task
      with:
        key_id: "{{ get_gcs_key_id.secret_value }}"
        secret_key: "{{ get_gcs_secret_key.secret_value }}"
    next:
      - step: download_and_convert

  - step: download_and_convert
    desc: "Download CSV from GS and convert to Parquet"
    call:
      type: workbook
      name: download_convert_task
      with:
        key_id: "{{ get_gcs_key_id.secret_value }}"
        secret_key: "{{ get_gcs_secret_key.secret_value }}"
    next:
      - step: create_postgres_table

  - step: create_postgres_table
    desc: "Create table in Postgres"
    call:
      type: workbook
      name: create_table_task
      with:
        key_id: "{{ get_gcs_key_id.secret_value }}"
        secret_key: "{{ get_gcs_secret_key.secret_value }}"
    next:
      - step: load_data_to_postgres

  - step: load_data_to_postgres
    desc: "Load data from Parquet to Postgres"
    call:
      type: workbook
      name: load_data_task
      with:
        key_id: "{{ get_gcs_key_id.secret_value }}"
        secret_key: "{{ get_gcs_secret_key.secret_value }}"
    next:
      - step: upload_parquet_to_gs

  - step: upload_parquet_to_gs
    desc: "Upload Parquet file to Google Storage bucket"
    call:
      type: workbook
      name: upload_parquet_task
      with:
        key_id: "{{ get_gcs_key_id.secret_value }}"
        secret_key: "{{ get_gcs_secret_key.secret_value }}"
    next:
      - step: advanced_file_operations

  - step: advanced_file_operations
    desc: "Advanced file operations with Duckdb"
    call:
      type: workbook
      name: advanced_file_operations_task
      with:
        key_id: "{{ get_gcs_key_id.secret_value }}"
        secret_key: "{{ get_gcs_secret_key.secret_value }}"
    next:
      - step: delete_secrets

  - step: delete_secrets
    desc: "Delete all secrets created during workflow"
    call:
      type: workbook
      name: delete_secrets_task
      with:
        execution_id: "{{ workload.execution_id }}"
    next:
      - step: end

  - step: end
    desc: "End of workflow"

workbook:
  - name: create_pg_secret_task
    type: duckdb
    with:
      pg_user: "{{ pg_user }}"
      pg_password: "{{ pg_password }}"
      execution_id: "{{ execution_id }}"
    commands: |
      -- Create a secret file for PostgreSQL credentials with execution_id in the name
      COPY (SELECT '{{ pg_user }}' AS username, '{{ pg_password }}' AS password) 
      TO '/tmp/pg_secret_{{ execution_id }}.csv' (FORMAT CSV, HEADER);

  - name: read_from_postgres_task
    type: duckdb
    with:
      pg_user: "{{ pg_user }}"
      pg_password: "{{ pg_password }}"
      execution_id: "{{ execution_id }}"
    commands: |
      -- Install and load PostgreSQL extension
      INSTALL postgres;
      LOAD postgres;

      -- Read credentials from secret file
      DROP TABLE IF EXISTS pg_credentials;
      CREATE TABLE pg_credentials AS
      SELECT * FROM read_csv_auto('/tmp/pg_secret_{{ execution_id }}.csv');

      -- Attach PostgreSQL database using credentials from secret file
      ATTACH 'dbname={{ workload.pg_db }} user=' || (SELECT username FROM pg_credentials LIMIT 1) || 
             ' password=' || (SELECT password FROM pg_credentials LIMIT 1) || 
             ' host={{ workload.pg_host }} port={{ workload.pg_port }}' 
      AS postgres_db (TYPE postgres);

      -- Read data from test_data_table
      DROP TABLE IF EXISTS temp_csv;
      CREATE TABLE temp_csv AS
      SELECT * FROM postgres_db.{{ workload.source_table_name }};

      -- Show the data
      SELECT * FROM temp_csv;
      DESCRIBE temp_csv;

      -- Save to local CSV file
      COPY temp_csv TO '{{ workload.local_csv_path }}' (FORMAT CSV, HEADER);

      -- Clean up
      DROP TABLE pg_credentials;
      DROP TABLE temp_csv;

  - name: delete_secrets_task
    type: duckdb
    with:
      execution_id: "{{ execution_id }}"
    commands: |
      -- Note: We can't delete the PostgreSQL secret file directly from DuckDB
      -- as it doesn't support file system operations like deleting files.
      -- The secret file /tmp/pg_secret_{{ execution_id }}.csv will remain on the file system.

      -- Drop DuckDB secrets
      DROP SECRET IF EXISTS gcs_access;

  - name: create_gcs_secret_task
    type: duckdb
    with:
      key_id: "{{ key_id }}"
      secret_key: "{{ secret_key }}"
    commands: |
      -- SET secret_directory = './data/noetldb/secrets';
      INSTALL httpfs;
      LOAD httpfs;

      DROP SECRET IF EXISTS gcs_access;

      SET s3_endpoint='storage.googleapis.com';
      SET s3_region='auto';
      SET s3_url_style='path';
      SET s3_use_ssl=true;

      CREATE PERSISTENT SECRET gcs_access (
          TYPE S3,
          KEY_ID '{{ key_id }}',
          SECRET '{{ secret_key }}',
          ENDPOINT 'storage.googleapis.com',
          REGION 'auto',
          URL_STYLE 'path',
          USE_SSL true
      );


  - name: test_gcs_credentials
    desc: "Test GCS credentials by attempting to list bucket or create a test file"
    type: duckdb
    commands: |
      -- Install and load necessary extensions
      INSTALL httpfs;
      LOAD httpfs;
      SET s3_endpoint='storage.googleapis.com';
      SET s3_region='auto';
      SET s3_url_style='path';
      SET s3_use_ssl=true;
      SET s3_access_key_id='{{ key_id }}';
      SET s3_secret_access_key='{{ secret_key }}';
      
      -- Create a simple test table to verify GCS write access
      DROP TABLE IF EXISTS gcs_test;
      CREATE TABLE gcs_test AS
      SELECT 'test' AS message, CURRENT_TIMESTAMP AS timestamp;
      
      -- Test by uploading a small test file to verify credentials work
      COPY gcs_test TO 'gs://{{ workload.gs_bucket }}/test_connection.csv' (FORMAT CSV, HEADER);
      
      -- Verify we can read it back
      SELECT 'GCS credentials test successful' AS status, * FROM read_csv_auto('gs://{{ workload.gs_bucket }}/test_connection.csv');
      
      -- Clean up
      DROP TABLE gcs_test;

  - name: upload_csv_task
    type: duckdb
    commands: |
      -- Install and load necessary extensions
      INSTALL httpfs;
      LOAD httpfs;

      SET s3_endpoint='storage.googleapis.com';
      SET s3_region='auto';
      SET s3_url_style='path';
      SET s3_use_ssl=true;
      SET s3_access_key_id='{{ key_id }}';
      SET s3_secret_access_key='{{ secret_key }}';

      -- Read CSV file from local filesystem using auto-detection
      DROP TABLE IF EXISTS temp_csv;
      CREATE TABLE temp_csv AS 
      SELECT * FROM read_csv_auto('{{ workload.source_csv_path }}', 
                                 all_varchar=false,  
                                 sample_size=-1);    

      -- Show the data and inferred types
      SELECT * FROM temp_csv;
      DESCRIBE temp_csv;

      -- Upload to Google Storage
      COPY temp_csv TO 'gs://{{ workload.gs_bucket }}/{{ workload.gs_csv_path }}' (FORMAT CSV, HEADER);

      -- Clean up
      DROP TABLE temp_csv;

  - name: download_convert_task
    type: duckdb
    commands: |
      -- Install and load necessary extensions
      INSTALL httpfs;
      LOAD httpfs;
      INSTALL parquet;
      LOAD parquet;

      SET s3_endpoint='storage.googleapis.com';
      SET s3_region='auto';
      SET s3_url_style='path';
      SET s3_use_ssl=true;
      SET s3_access_key_id='{{ key_id }}';
      SET s3_secret_access_key='{{ secret_key }}';

      -- Download CSV from Google Storage
      DROP TABLE IF EXISTS temp_csv;
      CREATE TABLE temp_csv AS 
      SELECT * FROM read_csv_auto('gs://{{ workload.gs_bucket }}/{{ workload.gs_csv_path }}', 
                                 sample_size=1000,    
                                 all_varchar=false);  

      -- Show the data and inferred schema
      SELECT * FROM temp_csv;
      DESCRIBE temp_csv;

      -- Convert to Parquet and save locally with compression
      COPY temp_csv TO '{{ workload.local_parquet_path }}' (
        FORMAT PARQUET, 
        COMPRESSION ZSTD,
        ROW_GROUP_SIZE 100000
      );

      -- Clean up
      DROP TABLE temp_csv;

  - name: create_table_task
    type: postgres
    with:
      db_host: "{{ workload.pg_host }}"
      db_port: "{{ workload.pg_port }}"
      db_user: "{{ workload.pg_user }}"
      db_password: "{{ workload.pg_password }}"
      db_name: "{{ workload.pg_db }}"
    commands: |
      -- SHOWCASE: This task uses direct authentication as an alternative to secrets
      -- The following SQL demonstrates direct connection to PostgreSQL
      -- without using secrets for authentication

      -- Drop table if it exists
      DROP TABLE IF EXISTS {{ workload.table_name }};

      -- Create table with appropriate columns based on test_data_table schema
      CREATE TABLE {{ workload.table_name }} (
        id INTEGER,
        name VARCHAR(100),
        age INTEGER,
        created_at TIMESTAMP,
        is_active BOOLEAN,
        meta_data JSONB,
        description TEXT
      );

  - name: load_data_task
    type: duckdb
    commands: |
      -- Install and load extensions
      INSTALL parquet;
      LOAD parquet;
      INSTALL postgres;
      LOAD postgres;

      -- Attach PostgreSQL database
      ATTACH 'dbname={{ workload.pg_db }} user={{ workload.pg_user }} password={{ workload.pg_password }} host={{ workload.pg_host }} port={{ workload.pg_port }}' AS postgres_db (TYPE postgres);

      -- Load Parquet file into DuckDB
      DROP TABLE IF EXISTS temp_parquet;
      CREATE TABLE temp_parquet AS
      SELECT * FROM read_parquet('{{ workload.local_parquet_path }}',
                                binary_as_string=true,
                                file_row_number=true);

      -- Show the data and schema
      SELECT * FROM temp_parquet;
      DESCRIBE temp_parquet;

      -- Get Parquet file metadata and schema
      SELECT * FROM parquet_metadata('{{ workload.local_parquet_path }}');
      SELECT * FROM parquet_schema('{{ workload.local_parquet_path }}');

      -- Insert data into PostgreSQL with proper type handling
      INSERT INTO postgres_db.{{ workload.table_name }}
      SELECT
        CASE WHEN id IS NULL THEN NULL ELSE id::INTEGER END,
        name,
        CASE WHEN age IS NULL OR TRIM(age::VARCHAR) = '' THEN NULL ELSE age::INTEGER END,
        CURRENT_TIMESTAMP AS created_at,
        TRUE AS is_active,
        CASE WHEN meta_data IS NULL OR TRIM(meta_data::VARCHAR) = '' THEN NULL ELSE meta_data::JSON END,
        description
      FROM temp_parquet;

      -- Verify data in PostgreSQL
      SELECT * FROM postgres_db.{{ workload.table_name }};

      -- Clean up
      DROP TABLE temp_parquet;

  - name: upload_parquet_task
    type: duckdb
    commands: |
      -- Install and load extensions
      INSTALL httpfs;
      LOAD httpfs;
      INSTALL parquet;
      LOAD parquet;
      SET s3_endpoint='storage.googleapis.com';
      SET s3_region='auto';
      SET s3_url_style='path';
      SET s3_use_ssl=true;
      SET s3_access_key_id='{{ key_id }}';
      SET s3_secret_access_key='{{ secret_key }}';

      -- Example 1: Reading multiple files
      DROP TABLE IF EXISTS sample_data;
      CREATE TABLE sample_data AS
      SELECT 1 AS id, 'Example 1' AS name
      UNION ALL
      SELECT 2 AS id, 'Example 2' AS name;

      COPY (SELECT * FROM sample_data WHERE id = 1) TO '/tmp/sample1.csv' (FORMAT CSV, HEADER);
      COPY (SELECT * FROM sample_data WHERE id = 2) TO '/tmp/sample2.csv' (FORMAT CSV, HEADER);

      DROP TABLE IF EXISTS multi_file_read;
      CREATE TABLE multi_file_read AS
      SELECT * FROM read_csv(['/tmp/sample1.csv', '/tmp/sample2.csv'], header = true);

      SELECT 'Reading multiple files:' AS operation, * FROM multi_file_read;

      -- Example 2: Reading compressed files
      COPY sample_data TO '/tmp/sample_compressed.csv.gz' (FORMAT CSV, HEADER);

      DROP TABLE IF EXISTS compressed_file_read;
      CREATE TABLE compressed_file_read AS
      SELECT * FROM '/tmp/sample_compressed.csv.gz';

      SELECT 'Reading compressed file:' AS operation, * FROM compressed_file_read;

      -- Example 3: Using glob patterns
      DROP TABLE IF EXISTS glob_pattern_read;
      CREATE TABLE glob_pattern_read AS
      SELECT * FROM read_csv('/tmp/sample*.csv', header = true);

      SELECT 'Reading with glob pattern:' AS operation, * FROM glob_pattern_read;

      -- Upload Parquet to Google Storage
      COPY sample_data TO 'gs://{{ workload.gs_bucket }}/{{ workload.gs_parquet_path }}' (FORMAT PARQUET);

      -- Clean up
      DROP TABLE sample_data;
      DROP TABLE multi_file_read;
      DROP TABLE compressed_file_read;
      DROP TABLE glob_pattern_read;


  - name: advanced_file_operations_task
    type: duckdb
    commands: |
      -- Install and load extensions
      INSTALL httpfs;
      LOAD httpfs;
      INSTALL parquet;
      LOAD parquet;
      SET s3_endpoint='storage.googleapis.com';
      SET s3_region='auto';
      SET s3_url_style='path';
      SET s3_use_ssl=true;
      SET s3_access_key_id='{{ key_id }}';
      SET s3_secret_access_key='{{ secret_key }}';

      -- Create test data
      DROP TABLE IF EXISTS test_data;
      CREATE TABLE test_data AS
      SELECT 1 AS id, 'Test 1' AS name, 100 AS value
      UNION ALL
      SELECT 2 AS id, 'Test 2' AS name, 200 AS value;

      -- Save as CSV
      COPY test_data TO '/tmp/advanced_test.csv' (FORMAT CSV, HEADER);

      -- Read using auto-detection
      DROP TABLE IF EXISTS auto_read;
      CREATE TABLE auto_read AS
      SELECT * FROM read_csv_auto('/tmp/advanced_test.csv');

      SELECT 'Advanced operations complete:' AS operation, * FROM auto_read;

      -- Clean up
      DROP TABLE test_data;
      DROP TABLE auto_read;
